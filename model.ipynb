{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "# from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from transformers import TFBertForSequenceClassification, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Yelp review dataset in JSON format\n",
    "data = []\n",
    "with open('yelp_dataset/yelp_academic_dataset_review.json', 'r', encoding='utf-8') as file:\n",
    "    for _ in range(10000):  # Load only 10,000 data points\n",
    "        line = file.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "# Convert the JSON data to a DataFrame\n",
    "yelp_data = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                    review_id                 user_id             business_id  \\\n",
       "0     KU_O5udG6zpxOg-VcAEodg  mh_-eMZ6K5RLWhZyISBhwA  XQfwVwDr-v0ZS3_CbbE5Xw   \n",
       "1     BiTunyQ73aT9WBnpR9DZGw  OyoGAe7OKpv6SyGZT5g77Q  7ATYjTIgM3jUlt4UM3IypQ   \n",
       "2     saUsX_uimxRlCVr67Z4Jig  8g_iMtfSiwikVnbP2etR0A  YjUWPpI6HXG530lwP-fb2A   \n",
       "3     AqPFMleE6RsU23_auESxiA  _7bHUi9Uuf5__HHc_Q8guQ  kxX2SOes4o-D3ZQBkiMRfA   \n",
       "4     Sx8TMOWLNuJBWer-0pcmoA  bcjbaE6dDog4jkNY91ncLQ  e4Vwtrqf-wpJfwesgvdgxQ   \n",
       "...                      ...                     ...                     ...   \n",
       "9995  ZcBtCA9jGhLfakf1jJ2BAg  yab1cq5yzrTHzoyz8LYqYQ  1-z7wd860Rii4kbEMCT8DA   \n",
       "9996  UIkEO-10J6Y99IhRqUflvg  lYAmgL_l7A3MPFYe1DYKrw  EpREWeEpmR8f1qLHzzF0AA   \n",
       "9997  S-NQM3Axcg8JS3MXHUIvyw  rE2WwfgJbYfvDwBlgq__dQ  dvidzWEPgTQPeBc8CUV2OQ   \n",
       "9998  ME79YrEhm2xe4IQy_0zkGw  OnIklvzKDpk1BduC84TrTA  2XYPFRm7teCUr3eGsB2-qw   \n",
       "9999  05oKtleZ-JFGD6qt47VQcg  p0MhNWkwOPBfRo8qUb7faw  rnaE88k8yV5pFAGJeiIK4Q   \n",
       "\n",
       "      stars  useful  funny  cool  \\\n",
       "0       3.0       0      0     0   \n",
       "1       5.0       1      0     1   \n",
       "2       3.0       0      0     0   \n",
       "3       5.0       1      0     1   \n",
       "4       4.0       1      0     1   \n",
       "...     ...     ...    ...   ...   \n",
       "9995    5.0       0      0     0   \n",
       "9996    5.0       0      1     0   \n",
       "9997    5.0       0      0     0   \n",
       "9998    5.0       0      0     0   \n",
       "9999    2.0       0      0     0   \n",
       "\n",
       "                                                   text                 date  \n",
       "0     If you decide to eat here, just be aware it is...  2018-07-07 22:09:11  \n",
       "1     I've taken a lot of spin classes over the year...  2012-01-03 15:28:18  \n",
       "2     Family diner. Had the buffet. Eclectic assortm...  2014-02-05 20:30:30  \n",
       "3     Wow!  Yummy, different,  delicious.   Our favo...  2015-01-04 00:01:03  \n",
       "4     Cute interior and owner (?) gave us tour of up...  2017-01-14 20:54:15  \n",
       "...                                                 ...                  ...  \n",
       "9995  Excellent food and service. The place is funct...  2018-06-26 17:41:31  \n",
       "9996  Just about to get tucked into a meatloaf that ...  2018-01-09 20:26:13  \n",
       "9997  Outstanding customer service! And my car is dr...  2015-04-01 21:50:28  \n",
       "9998  I and my husband went here for Dinner one day ...  2015-06-08 19:32:26  \n",
       "9999  I saw Big Data when they came to town at the K...  2015-08-06 23:16:58  \n",
       "\n",
       "[10000 rows x 9 columns]>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp_data.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zz/vz7cvvnd5ws3182279mslrc80000gn/T/ipykernel_25933/1065921666.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  yelp_data['sentiment'] = yelp_data['stars'].apply(lambda x: 0 if x <= 2 else (1 if x == 3 else 2))\n"
     ]
    }
   ],
   "source": [
    "# Select the necessary features\n",
    "yelp_data = yelp_data[['text', 'stars']]\n",
    "\n",
    "# Map star ratings to sentiment labels (e.g., 0 for 1-2 stars, 1 for 3 stars, 2 for 4-5 stars)\n",
    "yelp_data['sentiment'] = yelp_data['stars'].apply(lambda x: 0 if x <= 2 else (1 if x == 3 else 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train, validation, and test sets\n",
    "train_data, test_data = train_test_split(yelp_data, test_size=0.2, random_state=42)\n",
    "val_data, test_data = train_test_split(test_data, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0d12ffd8efb4b1598870db572731b25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained BERT tokenizer and model in TensorFlow\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Unable to convert output to PyTorch tensors format, PyTorch is not installed.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/Users/nadunsenarathne/Downloads/Documents/IIT/3rd Year/1st Trimester/CM3604 Deep Learning/Course Work/cw2/model.ipynb Cell 7\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nadunsenarathne/Downloads/Documents/IIT/3rd%20Year/1st%20Trimester/CM3604%20Deep%20Learning/Course%20Work/cw2/model.ipynb#W5sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     labels \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(labels)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nadunsenarathne/Downloads/Documents/IIT/3rd%20Year/1st%20Trimester/CM3604%20Deep%20Learning/Course%20Work/cw2/model.ipynb#W5sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m input_ids, attention_masks, labels\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/nadunsenarathne/Downloads/Documents/IIT/3rd%20Year/1st%20Trimester/CM3604%20Deep%20Learning/Course%20Work/cw2/model.ipynb#W5sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m train_input_ids, train_attention_masks, train_labels \u001b[39m=\u001b[39m tokenize_data(train_data)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nadunsenarathne/Downloads/Documents/IIT/3rd%20Year/1st%20Trimester/CM3604%20Deep%20Learning/Course%20Work/cw2/model.ipynb#W5sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m val_input_ids, val_attention_masks, val_labels \u001b[39m=\u001b[39m tokenize_data(val_data)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nadunsenarathne/Downloads/Documents/IIT/3rd%20Year/1st%20Trimester/CM3604%20Deep%20Learning/Course%20Work/cw2/model.ipynb#W5sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m test_input_ids, test_attention_masks, test_labels \u001b[39m=\u001b[39m tokenize_data(test_data)\n",
      "\u001b[1;32m/Users/nadunsenarathne/Downloads/Documents/IIT/3rd Year/1st Trimester/CM3604 Deep Learning/Course Work/cw2/model.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nadunsenarathne/Downloads/Documents/IIT/3rd%20Year/1st%20Trimester/CM3604%20Deep%20Learning/Course%20Work/cw2/model.ipynb#W5sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m review \u001b[39m=\u001b[39m row[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nadunsenarathne/Downloads/Documents/IIT/3rd%20Year/1st%20Trimester/CM3604%20Deep%20Learning/Course%20Work/cw2/model.ipynb#W5sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m label \u001b[39m=\u001b[39m row[\u001b[39m'\u001b[39m\u001b[39msentiment\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/nadunsenarathne/Downloads/Documents/IIT/3rd%20Year/1st%20Trimester/CM3604%20Deep%20Learning/Course%20Work/cw2/model.ipynb#W5sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m encoded_data \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39;49mencode_plus(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nadunsenarathne/Downloads/Documents/IIT/3rd%20Year/1st%20Trimester/CM3604%20Deep%20Learning/Course%20Work/cw2/model.ipynb#W5sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     review,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nadunsenarathne/Downloads/Documents/IIT/3rd%20Year/1st%20Trimester/CM3604%20Deep%20Learning/Course%20Work/cw2/model.ipynb#W5sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     add_special_tokens\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nadunsenarathne/Downloads/Documents/IIT/3rd%20Year/1st%20Trimester/CM3604%20Deep%20Learning/Course%20Work/cw2/model.ipynb#W5sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     max_length\u001b[39m=\u001b[39;49m\u001b[39m128\u001b[39;49m,  \u001b[39m# You can adjust the maximum sequence length\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nadunsenarathne/Downloads/Documents/IIT/3rd%20Year/1st%20Trimester/CM3604%20Deep%20Learning/Course%20Work/cw2/model.ipynb#W5sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     padding\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mmax_length\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nadunsenarathne/Downloads/Documents/IIT/3rd%20Year/1st%20Trimester/CM3604%20Deep%20Learning/Course%20Work/cw2/model.ipynb#W5sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     return_attention_mask\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nadunsenarathne/Downloads/Documents/IIT/3rd%20Year/1st%20Trimester/CM3604%20Deep%20Learning/Course%20Work/cw2/model.ipynb#W5sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     return_tensors\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nadunsenarathne/Downloads/Documents/IIT/3rd%20Year/1st%20Trimester/CM3604%20Deep%20Learning/Course%20Work/cw2/model.ipynb#W5sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     truncation\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nadunsenarathne/Downloads/Documents/IIT/3rd%20Year/1st%20Trimester/CM3604%20Deep%20Learning/Course%20Work/cw2/model.ipynb#W5sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nadunsenarathne/Downloads/Documents/IIT/3rd%20Year/1st%20Trimester/CM3604%20Deep%20Learning/Course%20Work/cw2/model.ipynb#W5sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m input_ids\u001b[39m.\u001b[39mappend(encoded_data[\u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nadunsenarathne/Downloads/Documents/IIT/3rd%20Year/1st%20Trimester/CM3604%20Deep%20Learning/Course%20Work/cw2/model.ipynb#W5sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m attention_masks\u001b[39m.\u001b[39mappend(encoded_data[\u001b[39m'\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2977\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2967\u001b[0m \u001b[39m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   2968\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   2969\u001b[0m     padding\u001b[39m=\u001b[39mpadding,\n\u001b[1;32m   2970\u001b[0m     truncation\u001b[39m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2974\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   2975\u001b[0m )\n\u001b[0;32m-> 2977\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_encode_plus(\n\u001b[1;32m   2978\u001b[0m     text\u001b[39m=\u001b[39;49mtext,\n\u001b[1;32m   2979\u001b[0m     text_pair\u001b[39m=\u001b[39;49mtext_pair,\n\u001b[1;32m   2980\u001b[0m     add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m   2981\u001b[0m     padding_strategy\u001b[39m=\u001b[39;49mpadding_strategy,\n\u001b[1;32m   2982\u001b[0m     truncation_strategy\u001b[39m=\u001b[39;49mtruncation_strategy,\n\u001b[1;32m   2983\u001b[0m     max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m   2984\u001b[0m     stride\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m   2985\u001b[0m     is_split_into_words\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[1;32m   2986\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m   2987\u001b[0m     return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[1;32m   2988\u001b[0m     return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[1;32m   2989\u001b[0m     return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[1;32m   2990\u001b[0m     return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[1;32m   2991\u001b[0m     return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[1;32m   2992\u001b[0m     return_offsets_mapping\u001b[39m=\u001b[39;49mreturn_offsets_mapping,\n\u001b[1;32m   2993\u001b[0m     return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[1;32m   2994\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   2995\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   2996\u001b[0m )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/tokenization_utils.py:722\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    719\u001b[0m first_ids \u001b[39m=\u001b[39m get_input_ids(text)\n\u001b[1;32m    720\u001b[0m second_ids \u001b[39m=\u001b[39m get_input_ids(text_pair) \u001b[39mif\u001b[39;00m text_pair \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 722\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprepare_for_model(\n\u001b[1;32m    723\u001b[0m     first_ids,\n\u001b[1;32m    724\u001b[0m     pair_ids\u001b[39m=\u001b[39;49msecond_ids,\n\u001b[1;32m    725\u001b[0m     add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m    726\u001b[0m     padding\u001b[39m=\u001b[39;49mpadding_strategy\u001b[39m.\u001b[39;49mvalue,\n\u001b[1;32m    727\u001b[0m     truncation\u001b[39m=\u001b[39;49mtruncation_strategy\u001b[39m.\u001b[39;49mvalue,\n\u001b[1;32m    728\u001b[0m     max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m    729\u001b[0m     stride\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m    730\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m    731\u001b[0m     return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[1;32m    732\u001b[0m     prepend_batch_axis\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    733\u001b[0m     return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[1;32m    734\u001b[0m     return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[1;32m    735\u001b[0m     return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[1;32m    736\u001b[0m     return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[1;32m    737\u001b[0m     return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[1;32m    738\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    739\u001b[0m )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3467\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.prepare_for_model\u001b[0;34m(self, ids, pair_ids, add_special_tokens, padding, truncation, max_length, stride, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, prepend_batch_axis, **kwargs)\u001b[0m\n\u001b[1;32m   3464\u001b[0m \u001b[39mif\u001b[39;00m return_length:\n\u001b[1;32m   3465\u001b[0m     encoded_inputs[\u001b[39m\"\u001b[39m\u001b[39mlength\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(encoded_inputs[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m-> 3467\u001b[0m batch_outputs \u001b[39m=\u001b[39m BatchEncoding(\n\u001b[1;32m   3468\u001b[0m     encoded_inputs, tensor_type\u001b[39m=\u001b[39;49mreturn_tensors, prepend_batch_axis\u001b[39m=\u001b[39;49mprepend_batch_axis\n\u001b[1;32m   3469\u001b[0m )\n\u001b[1;32m   3471\u001b[0m \u001b[39mreturn\u001b[39;00m batch_outputs\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:223\u001b[0m, in \u001b[0;36mBatchEncoding.__init__\u001b[0;34m(self, data, encoding, tensor_type, prepend_batch_axis, n_sequences)\u001b[0m\n\u001b[1;32m    219\u001b[0m     n_sequences \u001b[39m=\u001b[39m encoding[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mn_sequences\n\u001b[1;32m    221\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_sequences \u001b[39m=\u001b[39m n_sequences\n\u001b[0;32m--> 223\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_to_tensors(tensor_type\u001b[39m=\u001b[39;49mtensor_type, prepend_batch_axis\u001b[39m=\u001b[39;49mprepend_batch_axis)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:712\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    710\u001b[0m \u001b[39melif\u001b[39;00m tensor_type \u001b[39m==\u001b[39m TensorType\u001b[39m.\u001b[39mPYTORCH:\n\u001b[1;32m    711\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_available():\n\u001b[0;32m--> 712\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mUnable to convert output to PyTorch tensors format, PyTorch is not installed.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    713\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m    715\u001b[0m     is_tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mis_tensor\n",
      "\u001b[0;31mImportError\u001b[0m: Unable to convert output to PyTorch tensors format, PyTorch is not installed."
     ]
    }
   ],
   "source": [
    "# Tokenize and preprocess the data\n",
    "def tokenize_data(data):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    labels = []\n",
    "\n",
    "    for index, row in data.iterrows():\n",
    "        review = row['text']\n",
    "        label = row['sentiment']\n",
    "\n",
    "        encoded_data = tokenizer.encode_plus(\n",
    "            review,\n",
    "            add_special_tokens=True,\n",
    "            max_length=128,  # You can adjust the maximum sequence length\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "        input_ids.append(encoded_data['input_ids'])\n",
    "        attention_masks.append(encoded_data['attention_mask'])\n",
    "        labels.append(label)\n",
    "\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    labels = torch.tensor(labels)\n",
    "\n",
    "    return input_ids, attention_masks, labels\n",
    "\n",
    "train_input_ids, train_attention_masks, train_labels = tokenize_data(train_data)\n",
    "val_input_ids, val_attention_masks, val_labels = tokenize_data(val_data)\n",
    "test_input_ids, test_attention_masks, test_labels = tokenize_data(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataloaders\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_dataset = TensorDataset(train_input_ids, train_attention_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "val_dataset = TensorDataset(val_input_ids, val_attention_masks, val_labels)\n",
    "val_sampler = SequentialSampler(val_dataset)\n",
    "val_dataloader = DataLoader(val_dataset, sampler=val_sampler, batch_size=batch_size)\n",
    "\n",
    "test_dataset = TensorDataset(test_input_ids, test_attention_masks, test_labels)\n",
    "test_sampler = SequentialSampler(test_dataset)\n",
    "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer and loss function\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "model.to(device)  # Move the model to the appropriate device (e.g., GPU if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "num_epochs = 3\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in tqdm(train_dataloader, desc=f'Epoch {epoch + 1}'):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        input_ids, attention_mask, labels = batch\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    average_loss = total_loss / len(train_dataloader)\n",
    "    print(f'Average Loss: {average_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected object or value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/nadunsenarathne/Downloads/Documents/IIT/3rd Year/1st Trimester/CM3604 Deep Learning/Course Work/cw2/model.ipynb Cell 1\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nadunsenarathne/Downloads/Documents/IIT/3rd%20Year/1st%20Trimester/CM3604%20Deep%20Learning/Course%20Work/cw2/model.ipynb#W0sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nadunsenarathne/Downloads/Documents/IIT/3rd%20Year/1st%20Trimester/CM3604%20Deep%20Learning/Course%20Work/cw2/model.ipynb#W0sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# Sample loading of datasets\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/nadunsenarathne/Downloads/Documents/IIT/3rd%20Year/1st%20Trimester/CM3604%20Deep%20Learning/Course%20Work/cw2/model.ipynb#W0sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m df_business \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_json(\u001b[39mr\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m/yelp_dataset/yelp_academic_dataset_business.json\u001b[39;49m\u001b[39m\"\u001b[39;49m, lines\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nadunsenarathne/Downloads/Documents/IIT/3rd%20Year/1st%20Trimester/CM3604%20Deep%20Learning/Course%20Work/cw2/model.ipynb#W0sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m df_reviews \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_json(\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m/yelp_dataset/yelp_academic_dataset_review.json\u001b[39m\u001b[39m\"\u001b[39m, lines\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nadunsenarathne/Downloads/Documents/IIT/3rd%20Year/1st%20Trimester/CM3604%20Deep%20Learning/Course%20Work/cw2/model.ipynb#W0sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# For the sake of this example, let's assume the data is loaded as:\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nadunsenarathne/Downloads/Documents/IIT/3rd%20Year/1st%20Trimester/CM3604%20Deep%20Learning/Course%20Work/cw2/model.ipynb#W0sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# df_business = pd.DataFrame([json_1])\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nadunsenarathne/Downloads/Documents/IIT/3rd%20Year/1st%20Trimester/CM3604%20Deep%20Learning/Course%20Work/cw2/model.ipynb#W0sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# df_reviews = pd.DataFrame([json_3])\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nadunsenarathne/Downloads/Documents/IIT/3rd%20Year/1st%20Trimester/CM3604%20Deep%20Learning/Course%20Work/cw2/model.ipynb#W0sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nadunsenarathne/Downloads/Documents/IIT/3rd%20Year/1st%20Trimester/CM3604%20Deep%20Learning/Course%20Work/cw2/model.ipynb#W0sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# Merging datasets on business_id\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/util/_decorators.py:207\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[0;32m--> 207\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[39m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/json/_json.py:612\u001b[0m, in \u001b[0;36mread_json\u001b[0;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit, encoding, encoding_errors, lines, chunksize, compression, nrows, storage_options)\u001b[0m\n\u001b[1;32m    609\u001b[0m     \u001b[39mreturn\u001b[39;00m json_reader\n\u001b[1;32m    611\u001b[0m \u001b[39mwith\u001b[39;00m json_reader:\n\u001b[0;32m--> 612\u001b[0m     \u001b[39mreturn\u001b[39;00m json_reader\u001b[39m.\u001b[39;49mread()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/json/_json.py:744\u001b[0m, in \u001b[0;36mJsonReader.read\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    742\u001b[0m         data \u001b[39m=\u001b[39m ensure_str(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata)\n\u001b[1;32m    743\u001b[0m         data_lines \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 744\u001b[0m         obj \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_object_parser(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_combine_lines(data_lines))\n\u001b[1;32m    745\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    746\u001b[0m     obj \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_object_parser(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/json/_json.py:768\u001b[0m, in \u001b[0;36mJsonReader._get_object_parser\u001b[0;34m(self, json)\u001b[0m\n\u001b[1;32m    766\u001b[0m obj \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    767\u001b[0m \u001b[39mif\u001b[39;00m typ \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mframe\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 768\u001b[0m     obj \u001b[39m=\u001b[39m FrameParser(json, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\u001b[39m.\u001b[39;49mparse()\n\u001b[1;32m    770\u001b[0m \u001b[39mif\u001b[39;00m typ \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mseries\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m obj \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    771\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(dtype, \u001b[39mbool\u001b[39m):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/json/_json.py:880\u001b[0m, in \u001b[0;36mParser.parse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    878\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_parse_numpy()\n\u001b[1;32m    879\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 880\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_parse_no_numpy()\n\u001b[1;32m    882\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    883\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/json/_json.py:1133\u001b[0m, in \u001b[0;36mFrameParser._parse_no_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1129\u001b[0m orient \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39morient\n\u001b[1;32m   1131\u001b[0m \u001b[39mif\u001b[39;00m orient \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m   1132\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj \u001b[39m=\u001b[39m DataFrame(\n\u001b[0;32m-> 1133\u001b[0m         loads(json, precise_float\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprecise_float), dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1134\u001b[0m     )\n\u001b[1;32m   1135\u001b[0m \u001b[39melif\u001b[39;00m orient \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msplit\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m   1136\u001b[0m     decoded \u001b[39m=\u001b[39m {\n\u001b[1;32m   1137\u001b[0m         \u001b[39mstr\u001b[39m(k): v\n\u001b[1;32m   1138\u001b[0m         \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m loads(json, precise_float\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprecise_float)\u001b[39m.\u001b[39mitems()\n\u001b[1;32m   1139\u001b[0m     }\n",
      "\u001b[0;31mValueError\u001b[0m: Expected object or value"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "model.eval()\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "for batch in tqdm(val_dataloader, desc='Validation'):\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    input_ids, attention_mask, labels = batch\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predicted_labels = torch.argmax(logits, dim=1).tolist()\n",
    "    predictions.extend(predicted_labels)\n",
    "    true_labels.extend(labels.tolist())\n",
    "\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "report = classification_report(true_labels, predictions)\n",
    "\n",
    "print(f'Validation Accuracy: {accuracy:.4f}')\n",
    "print(report)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
